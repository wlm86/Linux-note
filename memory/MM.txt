1.zone



1. zone
1.1 zone type
enum zone_type {  
#ifdef CONFIG_ZONE_DMA   
    ZONE_DMA,//标记适合DMA的内存域。该区域的长度依赖于处理器的类型。在IA-32计算机上，一般的限制是16MB，这是由古老的ISA设备强加的边界，但更现代的计算机也可能受这一限制的影响   
#endif   
#ifdef CONFIG_ZONE_DMA32   
    ZONE_DMA32,//标记了使用32位地址字可寻址、适合DMA的内存域。显然只有在64位系统上两种DMA内存域才有差别。在32位系统上本内存域是空的。   
#endif   
    ZONE_NORMAL,//标记了可直接映射的内核段的普通内存域。这是在所有体系结构上保证都会存在的唯一内存域，但无法保证该地址范围对应了实际的物理内存。例如，如果AMD64系统有2G内存，那么所有内存都属于ZONE_DMA32范围，而ZONE_NORMAL则为空。   
#ifdef CONFIG_HIGHMEM   
    ZONE_HIGHMEM,//标记了超出内核段的物理内存。   
#endif   
    ZONE_MOVABLE,//它是一个虚拟内存域，在防止物理内存碎片的机制中需要使用该内存域，我会在后面的文章中讲解。   
    MAX_NR_ZONES//充当结束标记。在内核想要迭代系统中所有内存域时，会用到该变量。   
}; 

struct zone {
	/* Fields commonly accessed by the page allocator */
	unsigned long		pages_min, pages_low, pages_high;//如果空闲页多于pages_high，则内存域的状态时理想的；如果空闲页的数目低于pages_low，则内核开始将页换出到硬盘；如果空闲页低于pages_min，那么页回收工作的压力就比较大，因为内核中急需空闲页。
	unsigned long		lowmem_reserve[MAX_NR_ZONES];//分别为各种内存域指定了若干页，用于一些无论如何都不能失败的关键性内存分配。
#ifdef CONFIG_NUMA
	int node;
	/*
	 * zone reclaim becomes active if more unmapped pages exist.
	 */
	unsigned long		min_unmapped_pages;
	unsigned long		min_slab_pages;
	struct per_cpu_pageset	*pageset[NR_CPUS];
#else
	struct per_cpu_pageset	pageset[NR_CPUS];//这个数组用于实现每个CPU的热/冷页帧列表。内核使用这些列表来保存可用于满足实现的“新鲜”页。但冷热页帧对应的高速缓存状态不同：有些页帧很可能在高速缓存中，因此可以快速访问，故称之为热的；未缓存的页帧与此相对，称之为冷的。
#endif
	/*
	 * free areas of different sizes
	 */
	spinlock_t		lock; //防止其它CPU访问统一个zone struct
#ifdef CONFIG_MEMORY_HOTPLUG
	/* see spanned/present_pages for more description */
	seqlock_t		span_seqlock;
#endif
	struct free_area	free_area[MAX_ORDER];//用于实现伙伴系统，每个数组元素都表示某种固定长度的一些连续内存区，对于包含在每个区域中的空闲内存页的管理，free_area是一个起点。
#ifndef CONFIG_SPARSEMEM
	/*
	 * Flags for a pageblock_nr_pages block. See pageblock-flags.h.
	 * In SPARSEMEM, this map is stored in struct mem_section
	 */
	unsigned long		*pageblock_flags;
#endif /* CONFIG_SPARSEMEM */
	ZONE_PADDING(_pad1_)
	//这一部分涉及的结构成员，用来根据活动情况对内存域中使用的页进行编目，如果页访问频繁，则内核认为它是活动的；而不活动的页则显然相反。在需要换出页时，这种区别是很重要的，如果可能的话，频繁使用的页应该保持不动，而多余的不活动的页则可以换出而没有什么影响。
	spinlock_t		lru_lock;//防止其它CPU访问统一个zone struct
	struct {
		struct list_head list;
		unsigned long nr_scan;
	} lru[NR_LRU_LISTS];

	struct zone_reclaim_stat reclaim_stat;
	unsigned long		pages_scanned;//指定了上次换出一页一来，有多少页未能成功扫描
	unsigned long		flags;//描述了内存域的当前状态
	/* Zone statistics */
	atomic_long_t		vm_stat[NR_VM_ZONE_STAT_ITEMS];//维护了大量有关该内存域的统计信息
	int prev_priority;//存储了上一次扫描操作扫描该内存域的优先级
	/*
	 * The target ratio of ACTIVE_ANON to INACTIVE_ANON pages on
	 * this zone's LRU.  Maintained by the pageout code.
	 */
	unsigned int inactive_ratio;

	ZONE_PADDING(_pad2_)
	/* Rarely used or read-mostly fields */
	//一下三个变量实现了一个等待队列，可用于等待某一页变为可用的进程，进程排成一个队列，等待某些条件，在条件变为真时，内核会通知进程恢复工作。
	wait_queue_head_t	* wait_table;
	unsigned long		wait_table_hash_nr_entries;
	unsigned long		wait_table_bits;
	/*
	 * Discontig memory support fields.
	 */
	struct pglist_data	*zone_pgdat;//建立内存域和父结点之间的关联
	/* zone_start_pfn == zone_start_paddr >> PAGE_SHIFT */
	unsigned long		zone_start_pfn;//内存域第一个页帧的索引
	unsigned long		spanned_pages;//指定内存域中页的总数，但并非所有的都可用，因为有空洞
	unsigned long		present_pages;//指定了内存域中实际上可用的页数目
	const char		*name;//保存该内存域的惯用名称，目前有3个选项可用NORMAL DMA HIGHMEM
}____cacheline_internodealigned_in_smp;



#if defined(CONFIG_SMP)
struct zone_padding {
	char x[0];
} ____cacheline_internodealigned_in_smp; // 用以实现最优的高速缓存对齐方式
// 成填充字段添加到结构中，以确保每个自旋锁都处于自身的缓存行中
#define ZONE_PADDING(name)	struct zone_padding name;
#else
#define ZONE_PADDING(name)
#endif

1.2 node 初期化

start_kernel()->setup_arch()->paging_init()->zone_sizes_init()->free_area_init_nodes()->free_area_init_node()

void __paginginit free_area_init_node(int nid, unsigned long *zones_size,
		unsigned long node_start_pfn, unsigned long *zholes_size)
{
	pg_data_t *pgdat = NODE_DATA(nid);

	pgdat->node_id = nid;//节点ID
	pgdat->node_start_pfn = node_start_pfn;//物理内存起始地址的页帧号
	calculate_node_totalpages(pgdat, zones_size, zholes_size);

	alloc_node_mem_map(pgdat);
#ifdef CONFIG_FLAT_NODE_MEM_MAP
	printk(KERN_DEBUG "free_area_init_node: node %d, pgdat %08lx, node_mem_map %08lx\n",
		nid, (unsigned long)pgdat,
		(unsigned long)pgdat->node_mem_map);
#endif
	//初始化pgdat下的各个zone及相关信息
	free_area_init_core(pgdat, zones_size, zholes_size);
}

static void __meminit calculate_node_totalpages(struct pglist_data *pgdat,
		unsigned long *zones_size, unsigned long *zholes_size)
{
	unsigned long realtotalpages, totalpages = 0;//该pgdata下的各个区(zone)所含页的页数
	enum zone_type i;

	for (i = 0; i < MAX_NR_ZONES; i++)
		totalpages += zone_spanned_pages_in_node(pgdat->node_id, i,
								zones_size);
	pgdat->node_spanned_pages = totalpages;

	realtotalpages = totalpages;
	//zone_absent_pages_in_node获取该pgdata下各个区(zone)所含的洞的页数，
	//对于连续型，实际上不存在“洞”
	for (i = 0; i < MAX_NR_ZONES; i++)
		realtotalpages -=
			zone_absent_pages_in_node(pgdat->node_id, i,
								zholes_size);
	pgdat->node_present_pages = realtotalpages;
	printk(KERN_DEBUG "On node %d totalpages: %lu\n", pgdat->node_id,
							realtotalpages);
}

static void __init_refok alloc_node_mem_map(struct pglist_data *pgdat)
{
	/* Skip empty nodes */
	if (!pgdat->node_spanned_pages)
		return;

#ifdef CONFIG_FLAT_NODE_MEM_MAP
	/* ia64 gets its own node_mem_map, before this, without bootmem */
	if (!pgdat->node_mem_map) {
		unsigned long size, start, end;
		struct page *map;

		/*
		 * The zone's endpoints aren't required to be MAX_ORDER
		 * aligned but the node_mem_map endpoints must be in order
		 * for the buddy allocator to function correctly.
		 */
		start = pgdat->node_start_pfn & ~(MAX_ORDER_NR_PAGES - 1);
		end = pgdat->node_start_pfn + pgdat->node_spanned_pages;
		end = ALIGN(end, MAX_ORDER_NR_PAGES);
		/*为了管理pglist所跨越的总的页数目，
		首先获得需要申请的struct page实例的内存大小*/
		size =  (end - start) * sizeof(struct page);
		map = alloc_remap(pgdat->node_id, size);
		if (!map)
			map = alloc_bootmem_node(pgdat, size);//依bootmem位图分配器申请内存
		pgdat->node_mem_map = map + (pgdat->node_start_pfn - start);
	}
#ifndef CONFIG_NEED_MULTIPLE_NODES
	/*
	 * With no DISCONTIG, the global mem_map is just set as node 0's
	 */
	if (pgdat == NODE_DATA(0)) {
		mem_map = NODE_DATA(0)->node_mem_map;
#ifdef CONFIG_ARCH_POPULATES_NODE_MAP
		if (page_to_pfn(mem_map) != pgdat->node_start_pfn)
			mem_map -= (pgdat->node_start_pfn - ARCH_PFN_OFFSET);
#endif /* CONFIG_ARCH_POPULATES_NODE_MAP */
	}
#endif
#endif /* CONFIG_FLAT_NODE_MEM_MAP */
}



/*
 * Set up the zone data structures:
 *   - mark all pages reserved
 *   - mark all memory queues empty
 *   - clear the memory bitmaps
 */
static void __paginginit free_area_init_core(struct pglist_data *pgdat,
		unsigned long *zones_size, unsigned long *zholes_size)
{
	enum zone_type j;
	int nid = pgdat->node_id;
	unsigned long zone_start_pfn = pgdat->node_start_pfn;
	int ret;

	pgdat_resize_init(pgdat);
	pgdat->nr_zones = 0;
	init_waitqueue_head(&pgdat->kswapd_wait);
	pgdat->kswapd_max_order = 0;
	pgdat_page_cgroup_init(pgdat);
	
	//依次建立pglist_data下的每个zone
	for (j = 0; j < MAX_NR_ZONES; j++) {
		struct zone *zone = pgdat->node_zones + j;
		unsigned long size, realsize, memmap_pages;
		enum lru_list l;
		
		//获取该区所跨越的页的总数
		size = zone_spanned_pages_in_node(nid, j, zones_size);
		
		//获取该区实际可用的物理页的总数（除去“洞”）
		realsize = size - zone_absent_pages_in_node(nid, j,
								zholes_size);

		/*
		 * Adjust realsize so that it accounts for how much memory
		 * is used by this zone for memmap. This affects the watermark
		 * and per-cpu initialisations
		 */
		//获取因管理该区所使用的struct page实例的内存大小
		memmap_pages =
			PAGE_ALIGN(size * sizeof(struct page)) >> PAGE_SHIFT;
		if (realsize >= memmap_pages) {
			//获取该区实际可用的物理页的总数（除去管理结构所占用页数）
			realsize -= memmap_pages;
			if (memmap_pages)
				printk(KERN_DEBUG
				       "  %s zone: %lu pages used for memmap\n",
				       zone_names[j], memmap_pages);
		} else
			printk(KERN_WARNING
				"  %s zone: %lu pages exceeds realsize %lu\n",
				zone_names[j], memmap_pages, realsize);

		/* Account for reserved pages */
		if (j == 0 && realsize > dma_reserve) {
			realsize -= dma_reserve;
			printk(KERN_DEBUG "  %s zone: %lu pages reserved\n",
					zone_names[0], dma_reserve);
		}
		
		//将非高端内存区中，还未被所占用的页数计入nr_kernel_pages
		if (!is_highmem_idx(j))
			nr_kernel_pages += realsize;
		nr_all_pages += realsize;//将所有还未被占用的页数计入nr_all_pages

		zone->spanned_pages = size;//将该区跨越的页数存入pglist_data下相应的
		////将该区可以使用的实际页数存入pglist_data下相应的
		zone->present_pages = realsize;
#ifdef CONFIG_NUMA
		zone->node = nid;
		zone->min_unmapped_pages = (realsize*sysctl_min_unmapped_ratio)
						/ 100;
		zone->min_slab_pages = (realsize * sysctl_min_slab_ratio) / 100;
#endif
		zone->name = zone_names[j];
		spin_lock_init(&zone->lock);
		spin_lock_init(&zone->lru_lock);
		zone_seqlock_init(zone);
		zone->zone_pgdat = pgdat;

		zone->prev_priority = DEF_PRIORITY;

		zone_pcp_init(zone);
		for_each_lru(l) {
			INIT_LIST_HEAD(&zone->lru[l].list);
			zone->lru[l].nr_scan = 0;
		}
		zone->reclaim_stat.recent_rotated[0] = 0;
		zone->reclaim_stat.recent_rotated[1] = 0;
		zone->reclaim_stat.recent_scanned[0] = 0;
		zone->reclaim_stat.recent_scanned[1] = 0;
		zap_zone_vm_stats(zone);
		zone->flags = 0;
		if (!size)
			continue;

		set_pageblock_order(pageblock_default_order());
		//将管理该zone中的pageblock的比特位图的起始地址存入zone->pageblock_flags
		setup_usemap(pgdat, zone, size);
		/*分配zone的hash资源（用于进程请求页时阻塞）;
		初始化zone的free_area，以及free_area元素下的各类free_list*/
		ret = init_currently_empty_zone(zone, zone_start_pfn,
						size, MEMMAP_EARLY);
		BUG_ON(ret);
		/*修正最高的页帧数highest_memap_pfn;
		获取zone所管理的页对应的struct page实例;
		在struct page中的flags中标注各种标志；
		将页所隶属的pageblock的位图标记为MIGRATE_MOVABLE;*/
		memmap_init(size, nid, j, zone_start_pfn);
		zone_start_pfn += size;
	}
}

static void __init setup_usemap(struct pglist_data *pgdat,
				struct zone *zone, unsigned long zonesize)
{
	unsigned long usemapsize = usemap_size(zonesize);/*每个zone中的页按pageblock被分成几个block，一个pageblock所含页数为(1 << (MAX_ORDER - 1)),每个pageblock需要几个bit位来存储信息（这几个bit位的作用，暂时不知道），usemap_size的作用就在于计算该zone中的pageblock数所对应的bit位数，并转化成字节数.*/
	zone->pageblock_flags = NULL;
	if (usemapsize)
		zone->pageblock_flags = alloc_bootmem_node(pgdat, usemapsize);/*将管理该zone中的pageblock的比特位图的起始地址存入zone->pageblock_flags*/
}

__meminit int init_currently_empty_zone(struct zone *zone,
					unsigned long zone_start_pfn,
					unsigned long size,
					enum memmap_context context)
{
	struct pglist_data *pgdat = zone->zone_pgdat;
	int ret;
	ret = zone_wait_table_init(zone, size);
	if (ret)
		return ret;
	pgdat->nr_zones = zone_idx(zone) + 1;//更新pgdat下的zone的数目

	zone->zone_start_pfn = zone_start_pfn;

	mminit_dprintk(MMINIT_TRACE, "memmap_init",
			"Initialising map node %d zone %lu pfns %lu -> %lu\n",
			pgdat->node_id,
			(unsigned long)zone_idx(zone),
			zone_start_pfn, (zone_start_pfn + size));

	zone_init_free_lists(zone);

	return 0;
}

static noinline __init_refok
int zone_wait_table_init(struct zone *zone, unsigned long zone_size_pages)
{
	int i;
	struct pglist_data *pgdat = zone->zone_pgdat;
	size_t alloc_size;

	/*
	 * The per-page waitqueue mechanism uses hashed waitqueues
	 * per zone.
	 */
	//获取所需的hash表的数组元素个数
	zone->wait_table_hash_nr_entries =
		 wait_table_hash_nr_entries(zone_size_pages);
	/*获取值wait_table_hash_nr_entries中首个bit位值为1的序号
	(从最低位0开始记起，例如1，则获取值为0)*/
	zone->wait_table_bits =
		wait_table_bits(zone->wait_table_hash_nr_entries);
	//获取所需的hash表的数组所需空间大小
	alloc_size = zone->wait_table_hash_nr_entries
					* sizeof(wait_queue_head_t);

	if (!slab_is_available()) {
		zone->wait_table = (wait_queue_head_t *)
			alloc_bootmem_node(pgdat, alloc_size);
	} else {
		/*
		 * This case means that a zone whose size was 0 gets new memory
		 * via memory hot-add.
		 * But it may be the case that a new node was hot-added.  In
		 * this case vmalloc() will not be able to use this new node's
		 * memory - this wait_table must be initialized to use this new
		 * node itself as well.
		 * To use this new node's memory, further consideration will be
		 * necessary.
		 */
		zone->wait_table = vmalloc(alloc_size);
	}
	if (!zone->wait_table)
		return -ENOMEM;

	for(i = 0; i < zone->wait_table_hash_nr_entries; ++i)
		init_waitqueue_head(zone->wait_table + i);

	return 0;
}

#define memmap_init(size, nid, zone, start_pfn) \
	memmap_init_zone((size), (nid), (zone), (start_pfn), MEMMAP_EARLY)


/*
 * Initially all pages are reserved - free ones are freed
 * up by free_all_bootmem() once the early boot process is
 * done. Non-atomic initialization, single-pass.
 */
void __meminit memmap_init_zone(unsigned long size, int nid, unsigned long zone,
		unsigned long start_pfn, enum memmap_context context)
{
	struct page *page;
	unsigned long end_pfn = start_pfn + size;
	unsigned long pfn;
	struct zone *z;

	if (highest_memmap_pfn < end_pfn - 1)
		highest_memmap_pfn = end_pfn - 1;//修正最高的页帧数

	z = &NODE_DATA(nid)->node_zones[zone];//获取需要操作的zone
	for (pfn = start_pfn; pfn < end_pfn; pfn++) {
		/*
		 * There can be holes in boot-time mem_map[]s
		 * handed to this function.  They do not
		 * exist on hotplugged memory.
		 */
		if (context == MEMMAP_EARLY) {
			if (!early_pfn_valid(pfn))
				continue;
			if (!early_pfn_in_nid(pfn, nid))
				continue;
		}
		page = pfn_to_page(pfn);//获取页帧号所对应的struct page实例地址
		set_page_links(page, zone, nid, pfn);
		mminit_verify_page_links(page, zone, nid, pfn);
		/*page的访问计数，当为0时，说明page是空闲的，
		当大于0的时候，说明page被一个或多个进程正在使用该页或者有进程在等待该页*/
		init_page_count(page);
		reset_page_mapcount(page);
		SetPageReserved(page);
		/*
		 * Mark the block movable so that blocks are reserved for
		 * movable at startup. This will force kernel allocations
		 * to reserve their blocks rather than leaking throughout
		 * the address space during boot when many long-lived
		 * kernel allocations are made. Later some blocks near
		 * the start are marked MIGRATE_RESERVE by
		 * setup_zone_migrate_reserve()
		 *
		 * bitmap is created for zone's valid pfn range. but memmap
		 * can be created for invalid pages (for alignment)
		 * check here not to call set_pageblock_migratetype() against
		 * pfn out of zone.
		 */
		if ((z->zone_start_pfn <= pfn)
		    && (pfn < z->zone_start_pfn + z->spanned_pages)
		    && !(pfn & (pageblock_nr_pages - 1)))
			set_pageblock_migratetype(page, MIGRATE_MOVABLE);

		INIT_LIST_HEAD(&page->lru);
#ifdef WANT_PAGE_VIRTUAL
		/* The shift won't overflow because ZONE_NORMAL is below 4G. */
		if (!is_highmem_idx(zone))
			set_page_address(page, __va(pfn << PAGE_SHIFT));
#endif
	}
}

1.3 page_min/page_low/page_high
   pages_low：当空闲页面达到pages_low时，buddy分配器唤醒kswapd守护进程来回收页面。默认值为pages_min的两倍；
pages_min：当空闲页面达到pages_min时，分配器就会唤醒kswapd以同步方式工作，有时称为直接回收（direct-reclaim）路径。
      pages_high：唤醒kswapd进程之后，空闲页面达到pages_high时，就不会认为需要平衡区域。当达到这个水准后，kswapd就会进入休眠状态；pages_high的默认值为pages_min的三倍。


1.4 page
 struct page {
	unsigned long flags;//存储了体系结构无关的标志，用于描述页的属性		
	atomic_t _count;//是一个使用计数，表示内核中应用该页的次数。在其值到达0时，内核就知道page实例当前不使用，因此可以删除；如果其值大于0，该实例绝不会从内存删除。
	union {
		atomic_t _mapcount;//内存管理子系统中映射的页表项计数，表示在页表中有多少项指向该页
		struct {		/* SLUB */
			u16 inuse;
			u16 objects;
		};//用于SLUB分配器，对象的数目
	};
	union {
	    struct {
		unsigned long private;//是一个指向“私有”数据的指针，虚拟内存管理会忽略该数据。根据页的用途，可以用不用的方式使用该指针，大多数情况下它用于将页与缓冲区关联起来。
		struct address_space *mapping;//mapping默认情况下是指向address_space的，但如果使用技巧将其最低位置1，mapping就指向anon_vma对象
	    };
#if NR_CPUS >= CONFIG_SPLIT_PTLOCK_CPUS
	    spinlock_t ptl;
#endif
	    struct kmem_cache *slab;//用于SLUB 分配器，指向slab的指针
	    struct page *first_page;//用于复合页的尾页，指向首页
	};
	union {
		pgoff_t index;//在映射内的偏移量
		void *freelist;		/* SLUB: freelist req. slab lock */
	};
	struct list_head lru;//用于在各种链表上维护该页，如next执行cache，pre指向slab.
#if defined(WANT_PAGE_VIRTUAL)
	void *virtual;//用于高端内存区域中的页，换言之，即无法直接映射到内核内存中的页，virtual用于存储该页的虚拟地址。
#endif /* WANT_PAGE_VIRTUAL */
};

